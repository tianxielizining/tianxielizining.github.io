<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>那些书们</title>
    <url>/2022/05/04/Books/</url>
    <content><![CDATA[<p>记录读过的书和想读的书～</p>
 <span id="more"></span>

<h3 id="四月份书单（已读完）"><a href="#四月份书单（已读完）" class="headerlink" title="四月份书单（已读完）"></a>四月份书单（已读完）</h3><p>《三体》刘慈欣</p>
<p>《三体II：黑暗森林》刘慈欣</p>
<p>《自律100天》杨晓霞</p>
<h3 id="五月份书单（已读完）"><a href="#五月份书单（已读完）" class="headerlink" title="五月份书单（已读完）"></a>五月份书单（已读完）</h3><p>《断舍离》 山下英子（日）</p>
<p>《出梁庄记》 梁鸿</p>
<p>《人间便利店》 村田沙耶香（日）</p>
<p>《山茶文具店》小川糸（日）</p>
<h3 id="我的书架"><a href="#我的书架" class="headerlink" title="我的书架"></a>我的书架</h3><p>《三体Ⅲ：死神永生》刘慈欣</p>
<p>《银河系搭车客指南》 道格拉斯·亚当斯（英）</p>
<p>《梁庄十年》 梁鸿</p>
<p>《你当像鸟飞往你的山》 塔拉·韦斯特弗（美）</p>
<p>《蛤蟆先生去看心理医生》 罗伯特·戴博德 （英）</p>
]]></content>
      <categories>
        <category>书单</category>
      </categories>
      <tags>
        <tag>Books</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅-机器学习课程-1.1</title>
    <url>/2022/05/04/ML_1/</url>
    <content><![CDATA[<p>本章为第一讲的学习笔记：深度学习基本概念简介（上），<a href="https://www.youtube.com/watch?v=Ye018rCVvOo">视频链接</a>，<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/regression%20(v16).pdf">slides</a>。</p>
 <span id="more"></span>

<p><strong>Main Idea：Machine Learning  $\approx$   Looking for function</strong></p>
<h3 id="Different-types-of-Functions"><a href="#Different-types-of-Functions" class="headerlink" title="Different types of Functions"></a>Different types of Functions</h3><ul>
<li><p>Regression (回归):  The function outputs a scalar. 例如预测未来某个时刻PM2.5的数值。</p>
</li>
<li><p>Classification: Given options (classes), the function outputs the correct one. 例如区分邮件是否是垃圾邮件。Alpha Go本质上也是个Classification问题，输入为棋盘上黑子和白子的位置，输出是从可以落子的地方选择一个可以落子的地方。</p>
</li>
<li><p>Structured Learning：create something with structure (image, document)。输出不只是输出一个数字，还要产生一个有结构的物件，比如一张图片，一篇文章。</p>
</li>
</ul>
<h3 id="How-to-find-a-function"><a href="#How-to-find-a-function" class="headerlink" title="How to find a function?"></a>How to find a function?</h3><ol>
<li><p>写出一个带有未知参数的函数：先猜测一下我们打算找的这个函数的数学式长啥样，猜测不一定是对的，可以以后去修正（例如 $7$ 天一循环）。带有未知参数的函数被称为Model。</p>
</li>
<li><p>从训练数据中定义 Loss ，Loss 也是一个 Function ，输入为Model里面的参数，输出代表如果把这一组未知的参数设定某一个数值的时候，这个数值好还是不好。可以定义：$L &#x3D; \frac{1} {N}\sum_{i}^{N} {e_i}$ ，其中 $e_i$ 代表第 $i$ 个真实值与预测值之间误差。$L$ 越大代表参数越不好，$L$ 越小代表参数越好。去试不同的参数可以得到等高线图，等高线图叫做 Error Surface 。</p>
<p>计算误差的方式：</p>
<ol>
<li>$e&#x3D;|y - \widehat{y}|$  对应的Loss叫mean absolute error (MAE)</li>
<li>$e &#x3D; (y - \widehat{y})^2$ 对应的Loss叫mean square error (MSE)</li>
<li>If $y$ and $\widehat{y}$ are both probability distributions $\rightarrow$ Cross entropy</li>
</ol>
</li>
<li><p>Optimization，解一个最优化问题。找参数使得计算的 $L$ 最小。用上标$*$来表示最好的参数。</p>
<p>如何找到最好的参数？使用方法：Gradient Descent，假设只有一个参数为$w$</p>
<ol>
<li><p>在 Error Surface 的横轴上随机选取一个初始的点 $w_0$。</p>
</li>
<li><p>计算 $\frac{ \partial L } { \partial w}|_ {w &#x3D; w_0}$ ，也就是切线斜率，如果斜率是负数，代表左边比较高，右边比较低，要想得到比较低的Loss，需要增大$w_0$；反之，左边比较低，右边比较高，需要减小$w_0$。增大和减小$w_0$的量取决于：斜率的大小（大则步幅大）以及 学习速率 （learning rate ， $\eta$ ，自己设定，也叫 hyperparameters（需要自己设定的参数）），量为 $ \eta \frac{ \partial L } { \partial w}|_ {w &#x3D; w_0}$，$w_1 &#x3D; w_0 - \eta \frac{ \partial L } { \partial w}|_ {w &#x3D; w_0}$。</p>
</li>
<li><p>Update $w$ Iterativey ，直到达到指定的计算次数（例100万次，也是一个hyperparameter） 或 斜率为$0$.</p>
</li>
</ol>
</li>
</ol>
<p>如上方法有一个缺点，即有多个斜率为$0$的点时，可能找不到最优解（找到的可能是local minima 而不是global minima）。但是，does local minima truly cause the problem？</p>
]]></content>
      <categories>
        <category>李宏毅-机器学习课程</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅-机器学习课程-1.2</title>
    <url>/2022/05/07/ML_2/</url>
    <content><![CDATA[<p>本章为第一讲的学习笔记：深度学习基本概念简介（下），<a href="https://www.youtube.com/watch?v=bHcJCp2Fyxs">视频链接</a>，<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/regression%20%28v16%29.pdf">slides</a>。</p>
 <span id="more"></span>

<p>Model的类型</p>
<ul>
<li><p>Linear model：</p>
<ul>
<li>$y &#x3D; b + wx_1$ (单个特征)   </li>
<li>$y &#x3D; b + \sum_j w_j x_j$ (多个特征)</li>
</ul>
<p><img src="/../image/image-20220507130047612.jpg" alt="image-20220507130047612"></p>
</li>
<li><p>Piecewise Linear model：可以用Sigmoid 或 Rectified Linear Unit (ReLU) 等 Activation Function 来表示。</p>
<ul>
<li><p>Sigmoid：$ sigmoid ( x ) &#x3D; \frac { 1 } {1 + e ^{ - x} }$.</p>
<ul>
<li>$ y &#x3D; b + \sum _ i c_i sigmoid (b_i + w_i x_1)$ (单个特征)  </li>
<li>$y &#x3D; b + \sum_i c_i sigmoid (b_i + \sum_j w_{ i j } x_j) $ (多个特征)</li>
</ul>
</li>
</ul>
<p><img src="/../image/db9.png" alt="image-20220507154018256"></p>
<ul>
<li><p>ReLU： ReLU和Sigmoid相比更加好。 </p>
<ul>
<li>$y &#x3D; b + \sum_{2i} c_i max(0, b_i +  w_i x_1)$  (单个特征)</li>
<li>$y &#x3D; b + \sum_{2i} c_i max(0, b_i + \sum_j w_{ij} x_j)$  （多个特征）这里 sum 的是 $2i$ 是因为两个 ReLU 才能合成一个 sigmoid。</li>
</ul>
<p><img src="/../image/image-20220507154700894.png" alt="image-20220507154700894"></p>
</li>
</ul>
</li>
</ul>
<h3 id="How-to-find-a-function"><a href="#How-to-find-a-function" class="headerlink" title="How to find a function?"></a>How to find a function?</h3><p>找function的方法和Liner Model的一样。</p>
<p>首先为了方便，把所有的未知参数用向量$\Theta $来表示，$\Theta$中包括了上式中的所有$b$, $c_i$, $b_i$, $w_i$。</p>
<ol>
<li><p>随机找一组$\Theta$ , 计作 $\Theta_0$.</p>
</li>
<li><p>计算Gradient，根据 Gradient 把 $\Theta_i$更新成$\Theta_ {i+1}$</p>
</li>
<li><p>重复步骤 2 直至指定的计算次数 或 算出的 Gradient 为 $0$ 向量.</p>
</li>
</ol>
<p>​     实际上，在做 Gradient Decent 的时候，会把原始数据分成一个一个的Batch。用第 $i$ 个 Batch 里的 Data 算 Loss ，然后更新 $\Theta _i$ ，然后用更新后的 $\Theta_i$  来对第  $i+1$  个 Batch 的 Data 算 Loss  然后继续更新。每一次更新参数叫做一次 Update 。遍历完所有的 Batch 之后叫做一个 Epoch 。Batch 的 Size 也是一个 hyperparameter 。</p>
<h3 id="一些新名字"><a href="#一些新名字" class="headerlink" title="一些新名字"></a>一些新名字</h3><p><img src="/../image/image-20220507155248015.png" alt="image-20220507155248015"></p>
<p>Neuron, Neural Network, hidden layer, DeepLearning 如图所示，其中$a_i &#x3D;  sigmoid (b_i + \sum_j w_{ ij } x_j )$ 或 $a_i &#x3D;  max(0, b_i + \sum_j w_{ij} x_j)$。</p>
<p>Overfitting：在训练资料上在变好，但是在实际上没有变好。</p>
]]></content>
      <categories>
        <category>李宏毅-机器学习课程</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅-机器学习课程-2.1</title>
    <url>/2022/05/24/ML_3/</url>
    <content><![CDATA[<p>本篇为第二讲的学习笔记：类神经网络训练不起来怎么办（1），<a href="https://www.youtube.com/watch?v=WeHM2xpYQpw">视频链接</a>，<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/overfit-v6.pdf">slides</a>。</p>
 <span id="more"></span>

<h3 id="机器学习任务攻略-如何更好得进行预测？"><a href="#机器学习任务攻略-如何更好得进行预测？" class="headerlink" title="机器学习任务攻略-如何更好得进行预测？"></a>机器学习任务攻略-如何更好得进行预测？</h3><h4 id="任务攻略"><a href="#任务攻略" class="headerlink" title="任务攻略"></a>任务攻略</h4><p><img src="/../image/1653378089149.jpg" alt="image-20220507130047612"></p>
<h4 id="攻略详解"><a href="#攻略详解" class="headerlink" title="攻略详解"></a>攻略详解</h4><ol>
<li><p>检查 loss on training data，看 model 在 training data 上有没有学起来。如果发现在 training data 上的 loss 很大，需要找 loss 大的原因。</p>
<ul>
<li><p>model bias （上节课所学的内容）</p>
<ul>
<li><p>model is too simple. 类似于大海捞针，针不在海里。model不好的话，无论怎么训练都得不到好的结果。</p>
</li>
<li><p>solution： 重新设计model，给予model更大的弹性。例如，可以增加输入的feature，如果前一天的信息不够多，可以用前一周的信息；也可以用deep learning 来增加更多的弹性。</p>
<img src="../image/1653379106850.jpg" alt="image-20220507130047612" style="zoom:22%;" /></li>
</ul>
</li>
<li><p>optimization </p>
<ul>
<li>例如 gradient descent 方法的不足，使得不能帮我们找到 loss 低的model。大海捞针，针在海里，但是我们并没有办法把它捞上来。</li>
<li>solution：More powerful optimization technology （next  lecture）</li>
</ul>
</li>
<li><p>如何判断是因为 model bias 还是因为 optimization？</p>
<ul>
<li>Gaining the insights from comparison<ul>
<li>建议先跑一些比较小的比较浅的network，甚至用一些不是deep learning的方法，比如说linear model，support vector machine。support vector machine 可能比较容易做optimize。</li>
<li>如果深的model和浅的model比起来，明明弹性更大，但是在训练集上表现效果更差，则说明是optimization的问题。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如果 loss on training data 小，但是 loss on testing data 大，可能遇到overfitting的问题</p>
<ul>
<li><p>Overfitting</p>
<ul>
<li><p>出现的原因，一个极端的例子：</p>
<img src="../image/1653381130331.jpg" alt="image-20220507130047612" style="zoom:52%;" />
</li>
<li><p>Solution： </p>
<ul>
<li>增加训练集</li>
<li>Data augmentation，用一些对于问题的理解创建资料，例如左右翻转，裁剪等等。</li>
<li>增加一些限制，例如限制为二次曲线。CNN会给定比较多的限制。但是限制多会产生model bias。需要做好 Bias-Complexity Trade-off<ul>
<li>Less parameters, sharing parameters。比较少的参数，让一些变量共用参数。</li>
<li>Less features</li>
<li>Early stopping </li>
<li>Regularization</li>
<li>Dropout</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>李宏毅-机器学习课程</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
